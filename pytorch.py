# -*- coding: utf-8 -*-
"""pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KPcqcruzqNRkaR-ZydfrGphJdlpSPoMK
"""

from google.colab import drive
import pandas as pd

drive.mount('/content/drive')
file_path = "/content/drive/MyDrive/Colab Notebooks/wine.csv"
#!ls "/content/drive/MyDrive/Colab Notebooks/"

df = pd.read_csv(file_path)
print(df.head())



import torch
import torchvision
from torch.utils.data import Dataset, DataLoader

import numpy as np
import math

class SoftDrinkDataset(Dataset):
  def __init__(self) -> None:
    # data loader
    data = np.loadtxt(file_path, delimiter= ",", dtype = np.float32, skiprows=1)
   #matrix tensors
    self.x =torch.from_numpy(data[:,1:]) #holds all feature vecotrs take all rows, all colums from index 1 and onward (col 1 is intended to be target rest are features) and convert to input tensor
    self.y=torch.from_numpy (data[:,[0]]) # holds all labels targets nsamples, 1
    self.n_samples=data.shape[0]
    self.n_features=data.shape[1]

  def __getitem__(self,index):
    return self.x[index], self.y[index]
  def __len__(self):
      return self.n_samples

dataset=SoftDrinkDataset()
first_data=dataset[3] # callls __getitem__(0) returns self.x[0], selfy[0]
features,labels=first_data
print(features)
print(labels)

ns=len(dataset)
print('number of samples: '+ str(ns))

#load dataset into minibatches, shuffle them in needed and load them in parallel using multiple worers

dataloader= DataLoader(dataset=dataset, batch_size=4, shuffle=True)

dataiter= iter(dataloader)
data=next(dataiter)
features,labels=data
print(features)
print(labels)

#mock data training
num_epochs=2
total_samples=len(dataset)
n_iterations = math.ceil(total_samples/4)
print(total_samples, n_iterations)

for epoch in range(num_epochs):
  for i, (inputs, labels) in enumerate(dataloader):
    #forward backward, update
    if (i+1)%5 == 0:
      print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')

X = dataset.x.numpy() # convert pytorch tensor into numpy array for basic checks
y = dataset.y.numpy().ravel()
print("X.shape:", X.shape)
print("y.shape:", y.shape)
print("Unique labels", np.unique(y, return_counts=True)) #making sure labels are from 1-3 and each are even count

bad_mask = ~(np.isfinite(X))   # True if value is NaN or Inf
print(bad_mask.sum(), "bad values found")

X_clean=[]
for row in X:
  if np.all(np.isfinite(row)):
    X_clean.append(row)

#standardization
from sklearn.preprocessing import StandardScaler

print("Mean per feature before scaling:", X.mean(axis=0))
print("Std per feature before scaling :", X.std(axis=0))

scaler=StandardScaler()

X_scaled=scaler.fit_transform(X)

print("Mean per feature after scaling:", X_scaled.mean(axis=0))
print("Std per feature after scaling :", X_scaled.std(axis=0))

from sklearn.decomposition import PCA
#PCA pre-reduction (reducing dimensions)
pca = PCA(n_components=13)
X_pca=pca.fit_transform(X_scaled)
print("Shape after PCA:", X_pca.shape)

from sklearn.manifold import TSNE

tsne = TSNE(n_components = 2, perplexity=20, n_iter=1000, random_state=42)

X_tsne = tsne.fit_transform(X_pca)
print(X_tsne.shape)

import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=dataset.y.numpy().ravel(), cmap='viridis', s=50)
plt.colorbar(label='SoftDrink Label')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.title('t-SNE Visualization of SoftDrink Dataset')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score



X_train, X_test, Y_train, Y_test, = train_test_split(X_tsne, y, test_size=0.3, random_state=42)

knn = KNeighborsClassifier(n_neighbors=31,  metric='manhattan')

knn.fit(X_train,Y_train)
y_pred = knn.predict(X_test)


accuracy = accuracy_score(Y_test, y_pred)
print("Test set accuracy:", accuracy)


from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(Y_test, y_pred)
print("Confusion Matrix:\n", cm)

report = classification_report(Y_test, y_pred)

print("Classification Report:\n", report)

knn = KNeighborsClassifier(n_neighbors=31, metric='minkowski', p=23)

knn.fit(X_train,Y_train)
y_pred = knn.predict(X_test)


accuracy = accuracy_score(Y_test, y_pred)
print("Test set accuracy:", accuracy)


from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(Y_test, y_pred)
print("Confusion Matrix:\n", cm)

report = classification_report(Y_test, y_pred)

print("Classification Report:\n", report)



knn = KNeighborsClassifier(n_neighbors=31)

knn.fit(X_train,Y_train)
y_pred = knn.predict(X_test)


accuracy = accuracy_score(Y_test, y_pred)
print("Test set accuracy:", accuracy)


from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(Y_test, y_pred)
print("Confusion Matrix:\n", cm)

report = classification_report(Y_test, y_pred)

print("Classification Report:\n", report)

import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=(8,6))
for label in np.unique(y):
  idx = np.where(y == label)
  plt.scatter(X_tsne[idx, 0], X_tsne[idx, 1], label=f'True {int(label)}', alpha=0.6)

plt.title("True Labels on t-SNE 2D embedding")
plt.xlabel("t-SNE Dimension 1")
plt.ylabel("t-SNE Dimension 2")
plt.legend()
plt.show()
